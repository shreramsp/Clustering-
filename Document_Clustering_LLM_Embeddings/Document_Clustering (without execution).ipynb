{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document Clustering with LLM Embeddings\n",
        "\n",
        "This notebook demonstrates document clustering using state-of-the-art LLM embeddings from sentence-transformers. We'll use pretrained transformer models to generate semantic embeddings and cluster documents based on their meaning.\n",
        "\n",
        "**Objective**: Cluster text documents using contextual embeddings from pretrained language models, capturing semantic similarity beyond simple keyword matching."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries\n",
        "!pip install sentence-transformers scikit-learn umap-learn matplotlib seaborn pandas numpy -q\n",
        "\n",
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Document Dataset\n",
        "\n",
        "Loading a subset of the 20 newsgroups dataset which contains posts from different newsgroup categories. Selecting 4 distinct categories for clustering."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading 20 newsgroups dataset with selected categories\n",
        "categories = [\n",
        "    'rec.sport.baseball',\n",
        "    'sci.space',\n",
        "    'comp.graphics',\n",
        "    'talk.politics.guns'\n",
        "]\n",
        "\n",
        "print(\"Loading 20 newsgroups dataset...\")\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=('headers', 'footers', 'quotes')  # Removing metadata\n",
        ")\n",
        "\n",
        "documents = newsgroups.data\n",
        "true_labels = newsgroups.target\n",
        "category_names = newsgroups.target_names\n",
        "\n",
        "# Taking subset for faster processing (300 documents)\n",
        "sample_size = 300\n",
        "indices = np.random.choice(len(documents), sample_size, replace=False)\n",
        "documents = [documents[i] for i in indices]\n",
        "true_labels = true_labels[indices]\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Number of documents: {len(documents)}\")\n",
        "print(f\"Categories: {category_names}\")\n",
        "print(f\"Category distribution: {np.bincount(true_labels)}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previewing Sample Documents\n",
        "\n",
        "Displaying sample documents from each category to observe the content and understand what we're clustering."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Previewing documents from each category\n",
        "print(\"=\"*80)\n",
        "print(\"DOCUMENT PREVIEW: Sample from Each Category\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for category_id, category_name in enumerate(category_names):\n",
        "    # Finding first document from this category\n",
        "    doc_indices = np.where(true_labels == category_id)[0]\n",
        "    if len(doc_indices) > 0:\n",
        "        sample_doc = documents[doc_indices[0]]\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Category: {category_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        # Showing first 500 characters\n",
        "        preview = sample_doc[:500].replace('\\n', ' ').strip()\n",
        "        print(f\"{preview}...\")\n",
        "        print(f\"\\n[Document length: {len(sample_doc)} characters]\")\n",
        "\n",
        "# Creating a summary DataFrame\n",
        "doc_df = pd.DataFrame({\n",
        "    'Document_ID': range(len(documents)),\n",
        "    'Category': [category_names[label] for label in true_labels],\n",
        "    'Length': [len(doc) for doc in documents],\n",
        "    'Preview': [doc[:100].replace('\\n', ' ')[:100] + '...' for doc in documents]\n",
        "})\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Document Summary Table (First 10 documents)\")\n",
        "print(f\"{'='*80}\")\n",
        "print(doc_df.head(10).to_string(index=False))"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pretrained LLM Embedding Model\n",
        "\n",
        "Loading the 'all-MiniLM-L6-v2' model from sentence-transformers. This is a lightweight but powerful model that generates 384-dimensional semantic embeddings."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pretrained sentence transformer model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "print(f\"Loading pretrained model: {model_name}...\")\n",
        "print(\"This may take a moment on first run as the model is downloaded.\")\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "print(f\"\\nModel loaded successfully!\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
        "print(f\"Max sequence length: {embedding_model.max_seq_length}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Document Embeddings\n",
        "\n",
        "Converting each document into a dense vector representation using the pretrained model. These embeddings capture semantic meaning and context."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating embeddings for all documents\n",
        "print(\"Generating embeddings for all documents...\")\n",
        "print(\"This may take 1-2 minutes depending on document length.\")\n",
        "\n",
        "embeddings = embedding_model.encode(\n",
        "    documents,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"\\nEmbeddings generated successfully!\")\n",
        "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
        "print(f\"Number of documents: {embeddings.shape[0]}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Showing sample embedding\n",
        "print(f\"\\nSample embedding (first 10 dimensions): {embeddings[0][:10]}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Document Embeddings\n",
        "\n",
        "Applying K-Means clustering on the document embeddings to group semantically similar documents together."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying K-Means clustering\n",
        "n_clusters = 4\n",
        "\n",
        "print(f\"Applying K-Means clustering with k={n_clusters}...\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Calculating metrics\n",
        "silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
        "ari = adjusted_rand_score(true_labels, cluster_labels)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Document Clustering Results\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Number of clusters: {n_clusters}\")\n",
        "print(f\"Cluster distribution: {np.bincount(cluster_labels)}\")\n",
        "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"Inertia: {kmeans.inertia_:.2f}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction for Visualization\n",
        "\n",
        "Reducing high-dimensional embeddings (384D) to 2D using UMAP for visualization while preserving the semantic structure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing dimensions for visualization using UMAP\n",
        "print(\"Reducing dimensions with UMAP for visualization...\")\n",
        "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "print(f\"Dimensionality reduction complete!\")\n",
        "print(f\"Original shape: {embeddings.shape}\")\n",
        "print(f\"Reduced shape: {embeddings_2d.shape}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Document Clusters\n",
        "\n",
        "Plotting the clustered documents in 2D space, comparing predicted clusters with true categories."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing clusters\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Subplot 1: Predicted clusters\n",
        "scatter1 = axes[0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                          c=cluster_labels, cmap='viridis', \n",
        "                          alpha=0.6, edgecolors='k', s=50)\n",
        "axes[0].set_title(f'Predicted Clusters (K-Means, k={n_clusters})\\nSilhouette: {silhouette_avg:.3f}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('UMAP Dimension 1', fontsize=12)\n",
        "axes[0].set_ylabel('UMAP Dimension 2', fontsize=12)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')\n",
        "\n",
        "# Subplot 2: True categories\n",
        "scatter2 = axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                          c=true_labels, cmap='plasma', \n",
        "                          alpha=0.6, edgecolors='k', s=50)\n",
        "axes[1].set_title(f'True Categories\\nARI: {ari:.3f}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('UMAP Dimension 1', fontsize=12)\n",
        "axes[1].set_ylabel('UMAP Dimension 2', fontsize=12)\n",
        "cbar = plt.colorbar(scatter2, ax=axes[1], label='Category ID')\n",
        "cbar.set_ticks(range(len(category_names)))\n",
        "cbar.set_ticklabels([name.split('.')[-1][:10] for name in category_names], fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Cluster Composition\n",
        "\n",
        "Examining which true categories ended up in each predicted cluster to understand the clustering quality."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing cluster composition\n",
        "print(f\"{'='*80}\")\n",
        "print(\"CLUSTER COMPOSITION ANALYSIS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Finding documents in this cluster\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_true_labels = true_labels[cluster_mask]\n",
        "    \n",
        "    # Counting categories in this cluster\n",
        "    print(f\"Total documents: {cluster_mask.sum()}\")\n",
        "    print(\"Category breakdown:\")\n",
        "    for cat_id, cat_name in enumerate(category_names):\n",
        "        count = (cluster_true_labels == cat_id).sum()\n",
        "        percentage = (count / cluster_mask.sum()) * 100 if cluster_mask.sum() > 0 else 0\n",
        "        print(f\"  {cat_name:30s}: {count:3d} ({percentage:5.1f}%)\")\n",
        "    \n",
        "    # Showing dominant category\n",
        "    if cluster_mask.sum() > 0:\n",
        "        dominant_cat = np.bincount(cluster_true_labels).argmax()\n",
        "        print(f\"Dominant category: {category_names[dominant_cat]}\")\n",
        "\n",
        "# Creating confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(true_labels, cluster_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=[f'Cluster {i}' for i in range(n_clusters)],\n",
        "            yticklabels=[name.split('.')[-1] for name in category_names])\n",
        "plt.title(f'Confusion Matrix: True Categories vs Predicted Clusters\\nARI: {ari:.3f}', \n",
        "         fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Cluster', fontsize=12)\n",
        "plt.ylabel('True Category', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Documents from Each Cluster\n",
        "\n",
        "Displaying representative documents from each cluster to understand what topics/themes were discovered."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing sample documents from each cluster\n",
        "print(f\"{'='*80}\")\n",
        "print(\"SAMPLE DOCUMENTS FROM EACH CLUSTER\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CLUSTER {cluster_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Finding documents in this cluster\n",
        "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "    \n",
        "    # Showing 2 sample documents\n",
        "    for i, doc_idx in enumerate(cluster_indices[:2]):\n",
        "        true_cat = category_names[true_labels[doc_idx]]\n",
        "        doc_preview = documents[doc_idx][:300].replace('\\n', ' ').strip()\n",
        "        \n",
        "        print(f\"\\nSample {i+1} (True category: {true_cat}):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{doc_preview}...\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Document clustering with LLM embeddings complete!\")\n",
        "print(f\"Achieved Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(f\"Achieved Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}