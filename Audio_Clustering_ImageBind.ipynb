{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B__RnQdO1_mC"
      },
      "source": [
        "# Audio Clustering with ImageBind LLM Embeddings\n",
        "\n",
        "This notebook demonstrates audio clustering using Meta's ImageBind model, a multimodal embedding model that creates joint representations across images, text, audio, and other modalities. We'll cluster environmental sound recordings using these powerful embeddings.\n",
        "\n",
        "**Objective**: Apply multimodal LLM embeddings to cluster audio data, demonstrating how ImageBind's unified embedding space enables unsupervised discovery of acoustic patterns and sound categories.\n",
        "\n",
        "**Note**: This notebook requires GPU runtime. Go to Runtime → Change runtime type → GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5fQ06c31_mD"
      },
      "outputs": [],
      "source": [
        "# Installing required libraries\n",
        "!pip install torch torchvision torchaudio transformers scikit-learn umap-learn matplotlib seaborn pandas librosa soundfile -q\n",
        "!pip install git+https://github.com/facebookresearch/ImageBind.git -q\n",
        "\n",
        "# Importing necessary libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix\n",
        "import umap\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Checking GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2WsHAh11_mE"
      },
      "source": [
        "## Downloading ESC-50 Dataset\n",
        "\n",
        "Downloading the ESC-50 (Environmental Sound Classification) dataset which contains 2000 audio recordings across 50 sound categories. We'll use a subset of diverse categories for clustering.\n",
        "\n",
        "**Note**: First download may take 3-5 minutes (~600MB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGsBa2wz1_mE"
      },
      "outputs": [],
      "source": [
        "# Downloading ESC-50 dataset\n",
        "print(\"Downloading ESC-50 dataset...\")\n",
        "print(\"This may take 3-5 minutes for first download.\")\n",
        "\n",
        "!wget -q https://github.com/karoldvl/ESC-50/archive/master.zip -O esc50.zip\n",
        "print(\"Download complete! Extracting...\")\n",
        "\n",
        "# Extracting the dataset\n",
        "with zipfile.ZipFile('esc50.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "print(\"Extraction complete!\")\n",
        "\n",
        "# Setting paths\n",
        "audio_path = 'ESC-50-master/audio/'\n",
        "meta_path = 'ESC-50-master/meta/esc50.csv'\n",
        "\n",
        "# Loading metadata\n",
        "metadata = pd.read_csv(meta_path)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Total audio files: {len(metadata)}\")\n",
        "print(f\"Number of categories: {metadata['target'].nunique()}\")\n",
        "print(f\"\\nFirst few rows of metadata:\")\n",
        "print(metadata.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKzCsfHg1_mE"
      },
      "source": [
        "## Selecting Diverse Sound Categories\n",
        "\n",
        "Selecting 10 diverse sound categories from different domains (animals, nature, indoor, urban) for clustering analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyTbAEm51_mE"
      },
      "outputs": [],
      "source": [
        "# Selecting 10 diverse categories for clustering\n",
        "selected_categories = [\n",
        "    'dog',           # Animals\n",
        "    'rooster',       # Animals\n",
        "    'rain',          # Nature\n",
        "    'sea_waves',     # Nature\n",
        "    'crackling_fire',# Nature\n",
        "    'clock_tick',    # Indoor\n",
        "    'keyboard_typing', # Indoor\n",
        "    'door_wood_knock', # Indoor\n",
        "    'car_horn',      # Urban\n",
        "    'siren'          # Urban\n",
        "]\n",
        "\n",
        "# Filtering metadata for selected categories\n",
        "filtered_metadata = metadata[metadata['category'].isin(selected_categories)].copy()\n",
        "\n",
        "# Sampling to balance (20 samples per category for faster processing)\n",
        "samples_per_category = 20\n",
        "sampled_metadata = filtered_metadata.groupby('category').head(samples_per_category).reset_index(drop=True)\n",
        "\n",
        "print(f\"Selected categories: {len(selected_categories)}\")\n",
        "print(f\"Total audio samples: {len(sampled_metadata)}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(sampled_metadata['category'].value_counts().sort_index())\n",
        "\n",
        "# Creating category to ID mapping\n",
        "category_to_id = {cat: idx for idx, cat in enumerate(sorted(selected_categories))}\n",
        "sampled_metadata['category_id'] = sampled_metadata['category'].map(category_to_id)\n",
        "\n",
        "print(f\"\\nCategory ID mapping:\")\n",
        "for cat, idx in sorted(category_to_id.items(), key=lambda x: x[1]):\n",
        "    print(f\"{idx}: {cat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUf87oyK1_mF"
      },
      "source": [
        "## Visualizing Sample Audio Spectrograms\n",
        "\n",
        "Displaying spectrograms (visual representations of audio frequencies over time) for sample sounds from each category to understand the acoustic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhGe-J5h1_mF"
      },
      "outputs": [],
      "source": [
        "# Visualizing sample spectrograms from each category\n",
        "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, category in enumerate(sorted(selected_categories)):\n",
        "    # Getting first audio file for this category\n",
        "    sample = sampled_metadata[sampled_metadata['category'] == category].iloc[0]\n",
        "    audio_file = os.path.join(audio_path, sample['filename'])\n",
        "\n",
        "    # Loading and processing audio\n",
        "    y, sr = librosa.load(audio_file, sr=22050, duration=5.0)\n",
        "\n",
        "    # Computing mel spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Plotting\n",
        "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[idx])\n",
        "    axes[idx].set_title(category.replace('_', ' ').title(), fontsize=10)\n",
        "    axes[idx].set_xlabel('Time (s)', fontsize=8)\n",
        "    axes[idx].set_ylabel('Frequency (Hz)', fontsize=8)\n",
        "\n",
        "plt.suptitle('Sample Spectrograms from Each Sound Category', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Spectrograms show frequency content over time for each sound type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxjzelsr1_mF"
      },
      "source": [
        "## Loading ImageBind Model\n",
        "\n",
        "Loading Meta's ImageBind model for generating multimodal embeddings. ImageBind learns joint representations that work across vision, audio, text, and other modalities.\n",
        "\n",
        "**Note**: First run downloads ~2GB model weights. This may take 5-10 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJoIXX--1_mF"
      },
      "outputs": [],
      "source": [
        "# Loading ImageBind model\n",
        "try:\n",
        "    from imagebind import data\n",
        "    from imagebind.models import imagebind_model\n",
        "    from imagebind.models.imagebind_model import ModalityType\n",
        "\n",
        "    print(\"Loading ImageBind model...\")\n",
        "    print(\"This may take several minutes on first run as the model is downloaded.\")\n",
        "\n",
        "    model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"\\nImageBind model loaded successfully!\")\n",
        "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
        "    USE_IMAGEBIND = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ImageBind: {e}\")\n",
        "    print(\"\\nImageBind may not be available. This is expected if installation failed.\")\n",
        "    print(\"For this demo, we'll use a simple audio feature extraction method instead.\")\n",
        "    USE_IMAGEBIND = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpY2iHGI1_mF"
      },
      "source": [
        "## Loading and Preprocessing Audio Files\n",
        "\n",
        "Loading all selected audio files and preparing them for ImageBind embedding extraction. Audio is resampled to 16kHz as expected by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExIDC8-H1_mF"
      },
      "outputs": [],
      "source": [
        "# Loading all audio files\n",
        "print(\"Loading audio files...\")\n",
        "\n",
        "audio_files = []\n",
        "labels = []\n",
        "\n",
        "for idx, row in sampled_metadata.iterrows():\n",
        "    audio_file = os.path.join(audio_path, row['filename'])\n",
        "    audio_files.append(audio_file)\n",
        "    labels.append(row['category_id'])\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"Loaded {len(audio_files)} audio file paths\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Unique labels: {np.unique(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAZcqHEi1_mF"
      },
      "source": [
        "## Generating Audio Embeddings\n",
        "\n",
        "Extracting embeddings from ImageBind model for all audio samples. Processing in batches to manage memory efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDlTlMwE1_mF"
      },
      "outputs": [],
      "source": [
        "# Generating embeddings\n",
        "if USE_IMAGEBIND:\n",
        "    print(\"Generating embeddings with ImageBind...\")\n",
        "    print(\"This may take 3-5 minutes depending on GPU.\")\n",
        "\n",
        "    embeddings_list = []\n",
        "    batch_size = 8  # Smaller batch for audio\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(audio_files), batch_size):\n",
        "            batch_files = audio_files[i:i+batch_size]\n",
        "\n",
        "            # Loading audio using ImageBind's data loader\n",
        "            inputs = {\n",
        "                ModalityType.AUDIO: data.load_and_transform_audio_data(batch_files, device)\n",
        "            }\n",
        "\n",
        "            # Getting embeddings\n",
        "            embeddings_batch = model(inputs)[ModalityType.AUDIO]\n",
        "            embeddings_list.append(embeddings_batch.cpu())\n",
        "\n",
        "            if (i // batch_size + 1) % 5 == 0:\n",
        "                print(f\"Processed {i + len(batch_files)}/{len(audio_files)} audio files...\")\n",
        "\n",
        "    embeddings = torch.cat(embeddings_list, dim=0).numpy()\n",
        "\n",
        "else:\n",
        "    # Fallback: Simple MFCC features\n",
        "    print(\"Using MFCC features as fallback...\")\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i, audio_file in enumerate(audio_files):\n",
        "        y, sr = librosa.load(audio_file, sr=22050, duration=5.0)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "        mfccs_mean = np.mean(mfccs, axis=1)\n",
        "        embeddings_list.append(mfccs_mean)\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Processed {i+1}/{len(audio_files)} audio files...\")\n",
        "\n",
        "    embeddings = np.array(embeddings_list)\n",
        "\n",
        "print(f\"\\nEmbeddings generated successfully!\")\n",
        "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXC4A1HJ1_mG"
      },
      "source": [
        "## Clustering Audio Embeddings\n",
        "\n",
        "Applying K-Means clustering on the audio embeddings to group acoustically similar sounds together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpY1Ivld1_mG"
      },
      "outputs": [],
      "source": [
        "# Applying K-Means clustering\n",
        "n_clusters = 10\n",
        "\n",
        "print(f\"Applying K-Means clustering with k={n_clusters}...\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Calculating metrics\n",
        "silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
        "ari = adjusted_rand_score(labels, cluster_labels)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Audio Clustering Results\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Number of clusters: {n_clusters}\")\n",
        "print(f\"Cluster distribution: {np.bincount(cluster_labels)}\")\n",
        "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"Inertia: {kmeans.inertia_:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkkJdPMb1_mG"
      },
      "source": [
        "## Dimensionality Reduction with UMAP\n",
        "\n",
        "Reducing high-dimensional audio embeddings to 2D for visualization while preserving the acoustic similarity structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwIhjdOs1_mG"
      },
      "outputs": [],
      "source": [
        "# Reducing dimensions for visualization\n",
        "print(\"Reducing dimensions with UMAP...\")\n",
        "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "print(f\"Dimensionality reduction complete!\")\n",
        "print(f\"Original shape: {embeddings.shape}\")\n",
        "print(f\"Reduced shape: {embeddings_2d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTL8_pIf1_mG"
      },
      "source": [
        "## Visualizing Audio Clustering Results\n",
        "\n",
        "Plotting the clustered audio samples in 2D space, comparing predicted clusters with true sound categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZIALiuG1_mG"
      },
      "outputs": [],
      "source": [
        "# Visualizing clusters\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Subplot 1: Predicted clusters\n",
        "scatter1 = axes[0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                          c=cluster_labels, cmap='tab10',\n",
        "                          alpha=0.6, edgecolors='k', s=50)\n",
        "axes[0].set_title(f'Predicted Clusters (K-Means, k={n_clusters})\\nSilhouette: {silhouette_avg:.3f}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('UMAP Dimension 1', fontsize=12)\n",
        "axes[0].set_ylabel('UMAP Dimension 2', fontsize=12)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')\n",
        "\n",
        "# Subplot 2: True categories\n",
        "scatter2 = axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                          c=labels, cmap='tab10',\n",
        "                          alpha=0.6, edgecolors='k', s=50)\n",
        "axes[1].set_title(f'True Sound Categories\\nARI: {ari:.3f}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('UMAP Dimension 1', fontsize=12)\n",
        "axes[1].set_ylabel('UMAP Dimension 2', fontsize=12)\n",
        "cbar = plt.colorbar(scatter2, ax=axes[1], label='Category ID')\n",
        "cbar.set_ticks(range(10))\n",
        "cbar.set_ticklabels([cat.replace('_', ' ')[:10] for cat in sorted(selected_categories)],\n",
        "                    fontsize=8, rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tR-DbP61_mG"
      },
      "source": [
        "## Confusion Matrix Analysis\n",
        "\n",
        "Examining how predicted clusters align with true sound categories to understand which sounds are acoustically similar and may be confused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qDU65kS1_mG"
      },
      "outputs": [],
      "source": [
        "# Creating confusion matrix\n",
        "cm = confusion_matrix(labels, cluster_labels)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[f'C{i}' for i in range(n_clusters)],\n",
        "            yticklabels=[cat.replace('_', ' ').title() for cat in sorted(selected_categories)])\n",
        "plt.title(f'Confusion Matrix: True Categories vs Predicted Clusters\\nARI: {ari:.3f}',\n",
        "         fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Cluster', fontsize=12)\n",
        "plt.ylabel('True Sound Category', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyzing cluster purity\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CLUSTER PURITY ANALYSIS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "id_to_category = {v: k for k, v in category_to_id.items()}\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_true_labels = labels[cluster_mask]\n",
        "\n",
        "    if len(cluster_true_labels) > 0:\n",
        "        dominant_class = np.bincount(cluster_true_labels).argmax()\n",
        "        purity = (cluster_true_labels == dominant_class).sum() / len(cluster_true_labels)\n",
        "\n",
        "        print(f\"Cluster {cluster_id}:\")\n",
        "        print(f\"  Size: {len(cluster_true_labels)}\")\n",
        "        print(f\"  Dominant category: {id_to_category[dominant_class]}\")\n",
        "        print(f\"  Purity: {purity:.1%}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Kg1H3b1_mH"
      },
      "source": [
        "## Analyzing Acoustic Similarities\n",
        "\n",
        "Identifying which sound categories are most acoustically similar based on clustering results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y77Gs0QB1_mH"
      },
      "outputs": [],
      "source": [
        "# Finding most confused category pairs\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ACOUSTIC SIMILARITY ANALYSIS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(\"Category pairs frequently grouped together (acoustically similar):\\n\")\n",
        "\n",
        "confusion_pairs = []\n",
        "for i in range(len(cm)):\n",
        "    for j in range(len(cm[0])):\n",
        "        if cm[i][j] > 5:  # At least 5 samples grouped together\n",
        "            confusion_pairs.append((id_to_category[i], j, cm[i][j]))\n",
        "\n",
        "# Grouping by cluster\n",
        "cluster_groups = {}\n",
        "for cat, clust, count in confusion_pairs:\n",
        "    if clust not in cluster_groups:\n",
        "        cluster_groups[clust] = []\n",
        "    cluster_groups[clust].append((cat, count))\n",
        "\n",
        "for clust_id, categories in sorted(cluster_groups.items()):\n",
        "    if len(categories) > 1:\n",
        "        cat_names = [f\"{cat} ({cnt})\" for cat, cnt in categories]\n",
        "        print(f\"Cluster {clust_id}: {', '.join(cat_names)}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Audio clustering with ImageBind embeddings complete!\")\n",
        "print(f\"Achieved Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(f\"Achieved Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lli1K71v1_mH"
      },
      "source": [
        "## Results Interpretation\n",
        "\n",
        "**Clustering Performance:**\n",
        "- **Adjusted Rand Index (ARI):** Measures agreement between predicted clusters and true categories. For audio data, scores above 0.4 indicate good acoustic similarity detection, as many sounds have overlapping spectral characteristics (e.g., dog bark vs. rooster crow both have sharp attacks; rain vs. waves both have continuous noise).\n",
        "- **Silhouette Score:** Indicates cluster separation quality. Audio clustering typically shows moderate scores (0.1-0.3) due to natural acoustic overlap between categories. Environmental sounds exist on a continuum rather than discrete buckets.\n",
        "\n",
        "**Why Audio Clustering is Challenging:**\n",
        "Audio data is inherently complex with overlapping features across categories. Factors like recording quality, background noise, and acoustic environment add variability. Some sounds share frequency patterns (keyboard typing vs. rain), temporal patterns (clock tick vs. water drops), or timbral qualities (fire crackling vs. sea waves).\n",
        "\n",
        "**ImageBind's Multimodal Advantage:**\n",
        "ImageBind's training across multiple modalities (vision, audio, text) enables it to learn semantic relationships beyond raw acoustic features. The model understands that \"dog\" and \"rooster\" are both animals, or \"rain\" and \"waves\" are both water-related, creating embeddings that capture both acoustic AND semantic similarity.\n",
        "\n",
        "**Key Takeaway:**\n",
        "Multimodal LLM embeddings from ImageBind successfully discovered acoustic clusters without labeled training, demonstrating the power of pretrained models for audio understanding. The clustering reveals both acoustic similarities (spectral patterns) and semantic relationships (conceptual groupings) learned from cross-modal training."
      ]
    }
  ]
}